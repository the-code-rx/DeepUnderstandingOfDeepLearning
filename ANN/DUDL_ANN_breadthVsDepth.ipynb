{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: ANNs\n",
    "### LECTURE: Model depth vs. breadth\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ViJutqaaNb2"
   },
   "source": [
    "# Import and organize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MU7rvmWuhjud"
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "import pandas as pd\n",
    "iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
    "\n",
    "# convert from pandas dataframe to tensor\n",
    "data = torch.tensor( iris[iris.columns[0:4]].values ).float()\n",
    "\n",
    "# transform species to number\n",
    "labels = torch.zeros(len(data), dtype=torch.long)\n",
    "# labels[iris.species=='setosa'] = 0 # don't need!\n",
    "labels[iris.species=='versicolor'] = 1\n",
    "labels[iris.species=='virginica'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCuMSE6baRar"
   },
   "source": [
    "# Construct and sanity-check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eZMzMLxfULjf"
   },
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "\n",
    "class ANNiris(nn.Module):\n",
    "  def __init__(self,nUnits,nLayers):\n",
    "    super().__init__()\n",
    "\n",
    "    # create dictionary to store the layers\n",
    "    self.layers = nn.ModuleDict()\n",
    "    self.nLayers = nUnits#nLayers#\n",
    "\n",
    "    ### input layer\n",
    "    self.layers['input'] = nn.Linear(4,nUnits)\n",
    "\n",
    "    ### hidden layers\n",
    "    for i in range(nLayers):\n",
    "      self.layers[f'hidden{i}'] = nn.Linear(nUnits,nUnits)\n",
    "\n",
    "    ### output layer\n",
    "    self.layers['output'] = nn.Linear(nUnits,3)\n",
    "\n",
    "\n",
    "  # forward pass\n",
    "  def forward(self,x):\n",
    "    # input layer (note: the code in the video omits the relu after this layer)\n",
    "    x = F.relu( self.layers['input'](x) )\n",
    "\n",
    "    # hidden layers\n",
    "    for i in range(self.nLayers):\n",
    "      x = F.relu( self.layers[f'hidden{i}'](x) )\n",
    "\n",
    "    # return output layer\n",
    "    x = self.layers['output'](x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1676840556776,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -540
    },
    "id": "2-GmvNgEYgHK",
    "outputId": "64aaf42f-9ea9-4c3f-d621-40ede6bdc0d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANNiris(\n",
       "  (layers): ModuleDict(\n",
       "    (input): Linear(in_features=4, out_features=12, bias=True)\n",
       "    (hidden0): Linear(in_features=12, out_features=12, bias=True)\n",
       "    (hidden1): Linear(in_features=12, out_features=12, bias=True)\n",
       "    (hidden2): Linear(in_features=12, out_features=12, bias=True)\n",
       "    (hidden3): Linear(in_features=12, out_features=12, bias=True)\n",
       "    (output): Linear(in_features=12, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate an instance of the model and inspect it\n",
    "nUnitsPerLayer = 12\n",
    "nLayers = 4\n",
    "net = ANNiris(nUnitsPerLayer,nLayers)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 878,
     "status": "error",
     "timestamp": 1676840557650,
     "user": {
      "displayName": "Mike X Cohen",
      "userId": "13901636194183843661"
     },
     "user_tz": -540
    },
    "id": "XwtrXLSNYyC8",
    "outputId": "61867ff5-2e94-4f91-ec8c-cde7008638fd"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hidden4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m tmpx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# run it through the DL\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmpx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# exam the shape of the output\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m( y\u001b[38;5;241m.\u001b[39mshape ), \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mANNiris.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# hidden layers\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnLayers):\n\u001b[0;32m---> 29\u001b[0m   x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu( \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m(x) )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# return output layer\u001b[39;00m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m](x)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/container.py:461\u001b[0m, in \u001b[0;36mModuleDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;129m@_copy_to_script_wrapper\u001b[39m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Module:\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hidden4'"
     ]
    }
   ],
   "source": [
    "# A quick test of running some numbers through the model.\n",
    "# This simply ensures that the architecture is internally consistent.\n",
    "\n",
    "\n",
    "# 10 samples, 4 dimensions\n",
    "tmpx = torch.randn(10,4)\n",
    "\n",
    "# run it through the DL\n",
    "y = net(tmpx)\n",
    "\n",
    "# exam the shape of the output\n",
    "print( y.shape ), print(' ')\n",
    "\n",
    "# and the output itself\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YL7cvyjUaXjc"
   },
   "source": [
    "# Create a function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cVD1nFTli7TO"
   },
   "outputs": [],
   "source": [
    "# a function to train the model\n",
    "\n",
    "def trainTheModel(theModel):\n",
    "\n",
    "  # define the loss function and optimizer\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.SGD(theModel.parameters(),lr=.01)\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # forward pass\n",
    "    yHat = theModel(data)\n",
    "\n",
    "    # compute loss\n",
    "    loss = lossfun(yHat,labels)\n",
    "\n",
    "    # backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "  # final forward pass to get accuracy\n",
    "  predictions = theModel(data)\n",
    "  predlabels = torch.argmax(predictions, axis=1)\n",
    "  acc = 100*torch.mean((predlabels == labels).float())\n",
    "\n",
    "  # total number of trainable parameters in the model\n",
    "  nParams = sum(p.numel() for p in theModel.parameters() if p.requires_grad)\n",
    "\n",
    "  # function outputs\n",
    "  return acc,nParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "41R4X0MCaxVc",
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hidden4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test the function once\u001b[39;00m\n\u001b[1;32m      3\u001b[0m numepochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2500\u001b[39m\n\u001b[0;32m----> 4\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrainTheModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# check the outputs\u001b[39;00m\n\u001b[1;32m      7\u001b[0m acc \u001b[38;5;66;03m# tuple containing (accuracy,nparams)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mtrainTheModel\u001b[0;34m(theModel)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# loop over epochs\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epochi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numepochs):\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m   \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m   yHat \u001b[38;5;241m=\u001b[39m \u001b[43mtheModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m   \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m   loss \u001b[38;5;241m=\u001b[39m lossfun(yHat,labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mANNiris.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# hidden layers\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnLayers):\n\u001b[0;32m---> 29\u001b[0m   x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu( \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m(x) )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# return output layer\u001b[39;00m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m](x)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/container.py:461\u001b[0m, in \u001b[0;36mModuleDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;129m@_copy_to_script_wrapper\u001b[39m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Module:\n\u001b[0;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hidden4'"
     ]
    }
   ],
   "source": [
    "# test the function once\n",
    "\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show accuracy as a function of model depth\n",
    "fig, ax = plt.subplots(1,figsize=(12,6))\n",
    "\n",
    "ax.plot(numunits,accuracies,'o-',markerfacecolor='w',markersize=9)\n",
    "ax.plot(numunits[[0,-1]],[33,33],'--',color=[.8,.8,.8])\n",
    "ax.plot(numunits[[0,-1]],[67,67],'--',color=[.8,.8,.8])\n",
    "ax.legend(numlayers)\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_xlabel('Number of hidden units')\n",
    "ax.set_title('Accuracy')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "```python id=\"St6NI4qBk4tO\"\n",
    "# Maybe it's simply a matter of more parameters -> better performance?\n",
    "\n",
    "# vectorize for convenience\n",
    "x = totalparams.flatten()\n",
    "y = accuracies.flatten()\n",
    "\n",
    "# correlation between them\n",
    "r = np.corrcoef(x,y)[0,1]\n",
    "\n",
    "# scatter plot\n",
    "plt.plot(x,y,'o')\n",
    "plt.xlabel('Number of parameters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Correlation: r=' + str(np.round(r,3)))\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ix4I-SgJzXX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmraVzTcJ0x1"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pml6nCTcAMWC"
   },
   "outputs": [],
   "source": [
    "# 1) Try it again with 1000 training epochs. Do the deeper models eventually learn?\n",
    "#\n",
    "# 2) The categories are coded a \"0\", \"1\", and \"2\". Is there something special about those numbers?\n",
    "#    Recode the labels to be, e.g., 5, 10, and 17. Or perhaps -2, 0, and 2. Is the model still able to learn?\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1V0mkFVCMTAd0iq30FIpXspezzE7PpVrX",
     "timestamp": 1616523401452
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1615925514977
    }
   ]
  },
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
